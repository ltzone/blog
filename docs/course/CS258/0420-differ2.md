---
title: ã€Information Theoryã€‘Differential Entropy 2
url: it-diff2
date: 2020-04-20 08:03:26
tags: 
- Information Theory

categories: 
- Courses

---

Week 8 of 2020 Spring.

<!--more-->

[toc]

## Correlated Gaussian
æˆ‘ä»¬è®¡ç®—ä¸¤ä¸ªéšæœºå˜é‡çš„äº’ä¿¡æ¯ï¼Œè¿™é‡Œä»¥è”åˆé«˜æ–¯åˆ†å¸ƒä¸ºä¾‹ã€‚
(Mutual information between correlated Gaussian random variables with correlation $\rho$ ) Let $(X, Y) \sim \mathcal{N}(0, K),$ where
$$
\boldsymbol{K}=\left[\begin{array}{cc}
\boldsymbol{\sigma}^{2} & \boldsymbol{\rho} \boldsymbol{\sigma}^{2} \\
\boldsymbol{\rho} \boldsymbol{\sigma}^{2} & \boldsymbol{\sigma}^{2}
\end{array}\right]
$$
$I(X ; Y) ?$

$$
\begin{array}{c}
h(X)=h(Y)=\frac{1}{2} \log 2 \pi e \sigma^{2} \\
h(X, Y)=\frac{1}{2} \log (2 \pi e)^{2}|K|=\frac{1}{2} \log (2 \pi e)^{2} \sigma^{4}\left(1-\rho^{2}\right) \\
I(X ; Y)=h(X)+h(Y)-h(X, Y)=-\frac{1}{2} \log \left(1-\rho^{2}\right)
\end{array}
$$
- $\rho=0, X$ and $Y$ are independent and $I$ is 0
- $\rho=\pm 1, X$ and $Y$ are perfectly correlated and $I$ is $\infty$

## Maximum Entropy with Constraints

$E(X^2),Var(X)$ç»™å®šçš„æƒ…å†µä¸‹ï¼Œé«˜æ–¯åˆ†å¸ƒæœ€å¤§åŒ–å¾®åˆ†ç†µã€‚

> - Let the random variable $X \in R$ have mean $\mu$ and variance $\sigma^{2}$. Then
>   $$
>   h(X) \leq \frac{1}{2} \log 2 \pi e \sigma^{2}
>   $$
>   with equality iff $X \sim \mathcal{N}\left(\boldsymbol{\mu}, \boldsymbol{\sigma}^{2}\right)$
> - Let the random variable $X \in R$ satisfy $E X^{2} \leq \sigma^{2} .$ Then
>   $$
>   h(x) \leq \frac{1}{2} \log 2 \pi e \sigma^{2}
>   $$
>   with equality iff $X \sim \mathcal{N}\left(0, \sigma^{2}\right)$

è¯æ˜åŸŸå¹³å‡åˆ†å¸ƒæœ€å¤§åŒ–ç¦»æ•£ç†µçš„è¯æ˜å¦‚ä¸‹ï¼Œæˆ‘ä»¬ç”¨ç›¸å¯¹ç†µæ¨å‡ºã€‚

1. Let $X_{G} \sim \mathcal{N}\left(\mu, \sigma^{2}\right) .$ Consider 
   $$\boldsymbol{D}\left(X \| X_{G}\right) \geqq \mathbf{0}$$
  Then
  $$
  \int f \log \frac{f}{g} \geq 0
  $$
  æŠŠå¯¹æ•°å‡½æ•°å±•å¼€ï¼Œç”±äº$g$æ˜¯é«˜æ–¯åˆ†å¸ƒï¼Œå¯ä»¥è¿›ä¸€æ­¥å±•å¼€ã€‚
  $$h(X)=h(f) \leq-\int f \log g=-\int f \log \frac{1}{\sqrt{2 \pi \sigma^{2}}}+f\left(-\frac{(x-\mu)^{2}}{2 \sigma^{2}}\right)$$
  ç”±äºå³ä¾§éƒ½æ˜¯å¸¸æ•°ï¼Œå¯ä»£å…¥åŒ–ç®€ã€‚
  $$h(X) \leq \frac{1}{2} \log 2 \pi \sigma^{2}+\frac{1}{2}=\frac{1}{2} \log 2 \pi e \sigma^{2}$$
2. $\operatorname{Var}(X)=E\left(X^{2}\right)-E(X)^{2} \leq \sigma^{2} \cdot \Rightarrow$ Case 1

ä½¿ç”¨è¿™ä¸¤ä¸ªç»“è®ºæ—¶ä¸€å®šè¦æ³¨æ„æ˜¯å¦å­˜åœ¨ç¡®å®šçš„å‡å€¼ã€æ–¹å·®æˆ–äºŒé˜¶çŸ©æ˜¯å¦å­˜åœ¨ä¸Šç•Œã€‚

## Maximum Entropy

æœ€å¤§ç†µåŸç†åœ¨ä¸åŒçš„é™åˆ¶ä¸‹å¯ä»¥å¾—åˆ°ä¸åŒçš„ç»“è®ºã€‚ï¼ˆè¯¦è§Cover Ch.12ï¼‰

Consider the following problem: Maximize the entropy $h(f)$ over all probability densities $f$ satisfying ï¼ˆ$++$æ¡ä»¶ï¼‰
1. $f(x) \geq 0,$ with equality outside the support éè´Ÿæ€§
2. $\int_{s} f(x) d x=1$ è§„èŒƒæ€§
3. $\int_{S} f(x) r_{i}(x) d x=\alpha_{i}$ for $1 \leq i \leq m .\left(r_{i}(x) \text { is a function of } \right) x$. Thus, $f$ is a density on support set $S$ meeting certain moment constraints $\alpha_{1}, \alpha_{2}, \ldots, \alpha_{m}$ å³æŸäº›å…³äº$x$çš„å‡½æ•°çš„å‡å€¼æ˜¯ä¸€å®šçš„ã€‚

> Theorem 12.1.1 (Maximum entropy distribution) Let 
> $$f^{*}(x)=f_{\lambda}(x)=e^{\lambda_{0}+\sum_{i=1}^{m} \lambda_{i} r_{i}(x)}$$
> $x \in S,$ where $\lambda_{0}, \ldots, \lambda_{m}$ are chosen so that $f^{*}$ satisfies $(++) .$ Then $f^{*}$ uniquely maximizes $h(f)$ over all probability densities $f$ satisfying constraints $(++)$
> æœ€å¤§ç†µçš„åˆ†å¸ƒæ˜¯$f^{*}$å–åˆ°çš„ï¼Œä½†å…¶ä¸­ä¸€äº›ç³»æ•°éœ€è¦é€šè¿‡$\lambda_i$ä½œä¸ºå¾…å®šç³»æ•°ï¼Œè¿˜éœ€æ›´å¤šæ¡ä»¶å¯ç¡®å®šå¾…å®šç³»æ•°ã€‚

**Proof.**
$$\begin{aligned}
h(g) &=-\int_{S} g \ln g \\
&=-\int_{S} g \ln \frac{g}{f^{*}} f^{*} \\
&=-D\left(g \| f^{*}\right)-\int_{S} g \ln f^{*} \\
& \stackrel{(a)}{\leq}-\int_{S} g \ln f^{*} \\
& \stackrel{(b)}{=}-\int_{S} g\left(\lambda_{0}+\sum \lambda_{i} r_{i}\right) \\
& \stackrel{(c)}{=}-\int_{S} f^{*}\left(\lambda_{0}+\sum \lambda_{i} r_{i}\right) \\
&=-\int_{S} f^{*} \ln f^{*} \\
&=h\left(f^{*}\right)
\end{aligned}$$

where (a) follows from the nonnegativity of relative entropy, (b) follows from the definition of $f^{*},$ and $(\mathrm{c})$ follows from the fact that both $f^{*}$ and $g$ satisfy the constraints. Note that equality holds in (a) if and only if $g(x)=f^{*}(x)$ for all $x,$ except for a set of measure $0,$ thus proving uniqueness.

The same approach holds for discrete entropies and for multivariate distributions.

**Examples.**
- Let $S=[a, b],$ with no other constraints. Then the maximum entropy distribution is the uniform distribution over this range.
- $S=[0, \infty)$ and $E X=\mu .$ Then the entropy-maximizing distribution is
  $$
  f(x)=\frac{1}{\mu} e^{-\frac{x}{\mu}}, \quad x \geq 0
  $$
- $S=(-\infty, \infty), E X=\alpha_{1},$ and $E X^{2}=\alpha_{2} .$ The maximum entropy distribution is $\mathcal{N}\left(\alpha_{1}, \alpha_{2}-\alpha_{1}^{2}\right)$

## Hadamard's Inequality

$K$ is a nonnegative definite symmetric $n \times n$ matrix. Let $|K|$ denote the determinant of $K$
> Theorem (Hadamard) $|K| \leq \prod K_{i i},$ with equality iff $K_{i j}=0, \quad i \neq j$

**Proof.**
Let $X \sim \mathcal{N}(0, K) .$ Then
$$
\frac{1}{2} \log (2 \pi e)^{n}|K|=h\left(X_{1}, X_{2}, \ldots, X_{n}\right) \leqq \Sigma h\left(X_{i}\right)=\sum_{i=1}^{n} \frac{1}{2} \log 2 \pi e\left|K_{i i}\right|
$$
with equality iff $\left.X_{1}, X_{2}, \ldots, X_{n} \text { are independent (i.e., } K_{i j}=0, i \neq j\right)$

ideaï¼šçŸ©é˜µè½¬åŒ–ä¸ºå¤šå…ƒé«˜æ–¯åˆ†å¸ƒï¼Œè”åˆåˆ†å¸ƒç†µå°äºè¾¹ç¼˜åˆ†å¸ƒç†µçš„å’Œã€‚
remarkï¼šç†µæ˜¯ä¸€ä¸ªåŸºç¡€çš„ç‰©ç†é‡ï¼Œå¯ä»¥ç”¨æ¥è¯æ˜å¾ˆå¤šä¸ç­‰å¼(Cover Ch 17.9, 17.10)ï¼Œæ¯”å¦‚ä¸€ç³»åˆ—æœ‰å…³æ­£å®šçŸ©é˜µçš„æ€§è´¨
- $\log |K|$ is concave
- $\log \left(\left|K_{n}\right| /\left|K_{n-p}\right|\right)$ is concave in $K_{n}$
- $\left|K_{n}\right| /\left|K_{n-1}\right|$ is concave in $K_{n}$

## Balanced Information Inequality

å¹³è¡¡ä¿¡æ¯ä¸ç­‰å¼ï¼šç¦»æ•£ç†µä¸å¾®åˆ†ç†µçš„åŒå’Œä¸åŒ
Differences between inequalities of the discrete entropy and differential entropy
- Both $H(X, Y) \leq H(X)+H(Y)$ and $h(X, Y) \leq h(X)+h(Y)$ are valid
- $H(X, Y) \geq H(X)$ but neither $h(X, Y) \geq h(X)$ nor $h(X, Y) \leq h(X)$ is valid
Take $H(X, Y, Z) \leq \frac{1}{4} H(X)+\frac{1}{2} H(Y, Z)+\frac{3}{4} H(Z, X)$ for example.
Count the weights of random variables $X, Y, Z$ in both sides $X: 1,1 ; Y: 1, \frac{1}{2} ; Z: 1, \frac{5}{4}$ å®šä¹‰$X,Y,Z$çš„å‡€æƒé‡ã€‚
The net weights of $X, Y, Z$ are $0, \frac{1}{2},-\frac{1}{4}$

æ¯”å¦‚ï¼Œä¸‹é¢çš„ä¸ç­‰å¼æ˜¯å¹³è¡¡çš„ï¼š
Balanced: If the net weights of $X, Y, Z$ are all zero.
$$
h(X, Y) \leq h(X)+h(Y) \text { and } h(X, Y, Z) \leq \frac{1}{2} h(X, Y)+\frac{1}{2} h(Y, Z)+\frac{1}{2} h(Z, X)
$$

> å¯¹æ›´ä¸ºä¸€èˆ¬çš„æƒ…å†µï¼Œ
> Let $[n]:=\{1,2, \ldots, n\} .$ For any $\alpha \subseteq[n],$ denote $\left(X_{i}: i \in \alpha\right)$ by $X_{\alpha} .$ For example, $\alpha=\{1,3,4\},$ we denote $X_{1}, X_{3}, X_{4}$ by $X_{(1,3,4)}$ for simplicity.
> - We could write any information inequality in the form $\Sigma_{\alpha} w_{\alpha} H\left(X_{\alpha}\right) \geq 0$ or $\Sigma_{\alpha} w_{\alpha} h\left(X_{\alpha}\right) \geq 0$
> - An information inequality is called balanced if for any $i \in[n]$, the net weight of $X_{i}$ is zero.
> - The linear continuous inequality $\Sigma_{\alpha} w_{\alpha} h\left(X_{\alpha}\right) \geq 0$ is valid if and only if its corresponding discrete counterpart $\Sigma_{\mathrm{g}} w_{\mathrm{g}} H\left(X_{\mathrm{g}}\right) \geq 0$ is valid and balanced.
> ç”±æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥å»ºç«‹å¾®åˆ†ç†µä¸ç­‰å¼å’Œç¦»æ•£ç†µä¸ç­‰å¼çš„å…³ç³»ã€‚è¿™ä¸ªä¸ç­‰å¼æ˜¯æ­£ç¡®çš„**å½“ä¸”ä»…å½“**å®ƒå¯¹åº”çš„ç¦»æ•£ç†µä¸ç­‰å¼æ˜¯æ­£ç¡®çš„ä¸”å¹³è¡¡çš„ã€‚

Ref: Balanced Information Inequalities, T. H. Chan, IEEE Transactions
on Information Theory, Vol. 49 , No. 12 , December 2003

## Hanâ€™s Inequality

Let $\left(X_{1}, X_{2}, \ldots, X_{n}\right)$ have a density, and for every $S \subseteq\{1,2, \ldots, n\},$ denoted by $X(S)$ the subset $\left\{X_{i}: i \in S\right\} .$ Let
$$
\begin{array}{c}
h_{k}^{(n)}=\frac{1}{\left(\begin{array}{c}
n \\
k
\end{array}\right)} \sum_{S:[S]=k} \frac{h(X(S))}{k} \\
g_{k}^{(n)}=\frac{1}{\left(\begin{array}{l}
n \\
k
\end{array}\right)} \sum_{S:|S|=k} \frac{h\left(X(S) | X\left(S^{c}\right)\right)}{k}
\end{array}
$$

ä»nä¸ªéšæœºå˜é‡ä¸­å–kä¸ªï¼Œæ±‚è”åˆç†µã€æ¡ä»¶ç†µã€‚

When $n=3$,
$$\begin{aligned}
h_{1}^{(3)} &=\frac{H\left(X_{1}\right)+H\left(X_{2}\right)+H\left(X_{3}\right)}{3} \geq h_{2}^{(3)}=\frac{H\left(X_{1}, X_{2}\right)+H\left(X_{2}, X_{3}\right)+H\left(X_{3}, X_{1}\right)}{3} \\
& \geq h_{3}^{(3)}=H\left(X_{1}, X_{2}, X_{3}\right)
\end{aligned}$$

$$\begin{aligned}
g_{1}^{(3)} &=\frac{H\left(X_{1} | X_{2}, X_{3}\right)+H\left(X_{2} | X_{1}, X_{3}\right)+H\left(X_{3} | X_{1}, X_{2}\right)}{3} \\
& \leq g_{2}^{(3)}=\frac{H\left(X_{1}, X_{2} | X_{3}\right)+H\left(X_{2}, X_{3} | X_{1}\right)+H\left(X_{3}, X_{1} | X_{2}\right)}{3} \\
& \leq g_{3}^{(3)}=H\left(X_{1}, X_{2}, X_{3}\right)
\end{aligned}$$

> Han's inequality:
> $h_{1}^{(n)} \geq h_{2}^{(n)} \ldots \geq h_{n}^{(n)}=H\left(X_{1}, X_{2}, \ldots, X_{n}\right)=g_{n}^{(n)} \geq \cdots \geq g_{2}^{(n)} \geq g_{1}^{(n)}$



## Information Heat


### Heat Equation

- Heat equation (Fourier): Let $x$ be the position and $t$ be the time, çƒ­ä¼ å¯¼æ–¹ç¨‹ã€‚**ï¼ˆå®ƒä¸é«˜æ–¯ä¿¡é“æ˜¯ç­‰ä»·çš„ï¼‰**
  $$\frac{\partial}{\partial t} f(x, t)=\frac{1}{2} \frac{\partial^{2}}{\partial x^{2}} f(x, t)$$
- Let $X$ be any random variable with a density $f(x)$. Let $Z$ be an independent normally distributed random variable with zero mean and unit variance, $Z \sim \mathcal{N}(0,1) .$ Let
  $$Y_{t}=X+\sqrt{t} Z$$
  The **probability density function** $f(y ; t)(f(y ; t) \text { is a function in } y, \text { not } t)$ of $Y_{t}$ **satisfies heat equation**
  $$f(y ; t)=\int f(x) \frac{1}{\sqrt{2 \pi t}} e^{\frac{(y-x)^{2}}{2 t}} d x$$
  é«˜æ–¯ä¿¡é“çš„è¾“å‡ºä¿¡å·ä¸çƒ­ä¼ å¯¼æ–¹ç¨‹å…·æœ‰ä¸€ä¸€å¯¹åº”çš„å…³ç³»ã€‚

### Entropy and Fisher Information
å¯¹è¿ç»­å‹éšæœºå˜é‡å®šä¹‰ä¸€ä¸ªæ–°çš„ä¿¡æ¯é‡ï¼Œ Fisher Information

> Fisher information: Let $X$ be any random variable with density $f(x)$. Its Fisher information is given by
> $$
> I(x)=\int_{-\infty}^{+\infty} f(x)\left[\frac{\frac{\partial}{\partial x} f(x)}{f(x)}\right]^{2} d x
> $$
- Let $X$ be any random variable with a density $f(x)$. Let $Z$ be an independent normally distributed random variable with zero mean and unit variance. Let $Y_{t}=X+\sqrt{t} Z$
  $$\frac{\partial}{\partial t} h\left(Y_{t}\right)=\frac{1}{2} I\left(Y_{t}\right)$$ è¡¨æ˜ä¿¡æ¯é‡ä¸ç»Ÿè®¡é‡ä¹‹é—´ä¹Ÿå­˜åœ¨å…³è”ã€‚
- Let $f(y, t) \text { (or } f)$ be the p.d.f of $Y_{t}$
  $$\begin{array}{c}
  \frac{\partial}{\partial t} h\left(Y_{t}\right)=\frac{1}{2} I\left(Y_{t}\right)=\frac{1}{2} \int \frac{f_{y}^{2}}{f} d y \geq 0 \\
  \frac{\partial^{2}}{\partial t^{2}} h\left(Y_{t}\right)=-\frac{1}{2} \int f\left(\frac{f_{y y}}{f}-\frac{f_{y}^{2}}{f^{2}}\right)^{2} d y \leq 0
  \end{array}$$
  $h(Y_t)$å…³äºtæ˜¯ä¸€ä¸ªé€’å¢çš„å‡¹å‡½æ•°ã€‚

- When $X$ is Gaussian $\mathcal{N}(0,1)$
  $$h\left(Y_{t}\right)=\frac{1}{2} \log 2 \pi e(1+t)$$
  å¯¹é«˜æ–¯åˆ†å¸ƒçš„è¾“å…¥ï¼Œné˜¶å¯¼æ•°å¯æ±‚ï¼Œä¸”ç¬¦å·ä¸º
  All the derivatives alternate in signs: $+,-,+,-, \dots$

### Higher Order Derivatives of â„(ğ‘Œğ‘¡)

(Cheng 2015) Let $X$ be any random variable with a density $f(x)$. Let $Z$ be an independent normally distributed random variable with zero mean and unit variance. Let $Y_{t}=X+\sqrt{t} Z$ and $f(y, t) \text { (or } f)$ be the p.d.f of $Y_{t} .$ Then
$$
\frac{\partial^{3}}{\partial t^{3}} h\left(Y_{t}\right) \geq 0 \text { and } \frac{\partial^{4}}{\partial t^{4}} h\left(Y_{t}\right) \leq 0
$$
Conjecture: When $n$ is even, $\frac{\partial^{n}}{\partial t^{n}} h\left(Y_{t}\right) \leq 0,$ otherwise $\frac{\partial^{n}}{\partial t^{n}} h\left(Y_{t}\right) \geq 0$

Ref: F. Cheng and Y. Geng, Higher Order Derivatives in Costa's Entropy Power Inequality

## EPI and FII

> (Shannon 1948, Entropy power inequality (EPI)) If $X$ and $Y$ are independent random $n$ -vectors with densities, then
> $$e^{\frac{2}{n} h(X+Y)} \geq e^{\frac{2}{n} h(X)}+e^{\frac{2}{n} h(Y)}$$
è‹¥å¯¹éšæœºå˜é‡ï¼š
$$e^{2 h(X+Y)} \geq e^{2 h(X)}+e^{2 h(Y)}$$
- ä¹Ÿå¯ä»¥äº’æ¨FIIä¸ç­‰å¼ï¼ŒFisher information inequality (FII)
  $$\frac{1}{I(X+Y)} \geq \frac{1}{I(X)}+\frac{1}{I(Y)}$$
- Most profound result in Shannon's 1948 paper
- EPI can imply some very fundamental results 
  - Uncertainty principle in quantom physics
  - Young's inequality 
  - Nash's inequality 
  - Cramer-Rao bound

References:
- T. Cover, Information theoretic inequalities, 1990
- O. Rioul , â€œInformation Theoretic Proofs of Entropy Power Inequalities,â€ 2011