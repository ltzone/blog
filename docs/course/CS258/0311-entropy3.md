---
title: Entropy (3)
url: it-entropy3
date: 2020-03-11 10:01:59
tags: 
- Information Theory

categories: 
- Courses

---

Week 2 of 2020 Spring. More about entropy

**KeyWords**: Entropy

<!-- more -->



## Independence Bound on Entropy

From intuition to math expression.
Let $X_1,X_2,\cdots,X_n$ be drawn according to $p(x_1,x_2,\cdots,x_n)$. Then
$$H\left(X_{1}, X_{2}, \ldots, X_{n}\right) \leq \sum_{i=1}^{n} H\left(X_{i}\right)$$
with equality if and only if the $X_i$ are independent.

**Pf.** by chain rule and conditioning reduces entropy
$$H\left(X_{1}, X_{2}, \ldots, X_{n}\right)=\sum_{i=1}^{n} H\left(X_{i} | X_{i-1}, \ldots, X_{1}\right) \leq \sum_{i=1}^{n} H\left(X_{i}\right)$$


## Markov Chain
$$ p(x,y,z) = p(x)p(y|x)p(z|y) \text{denoted as} X \rightarrow Y\rightarrow Z$$
i.e. $p(z|y,x)=p(z|y)$

Prop about markov chain
-  $X \rightarrow Y \rightarrow Z$ iff $X$ and $Z$ are conditionally independent given $Y$.
- æ—¶é—´å¯é€† $X \iff Y \iff Z$
  - an easy interpretation is that in the mutual information $I(X ; Z | Y)$, $X$ and $Z$ can be switched.
- ä»¿å°„ if$Z=f(Y)$ then $X \rightarrow Y \rightarrow Z$.
- ä½“ç°åœ¨ä¿¡æ¯åº¦é‡ä¸Š, if $X \rightarrow Y\rightarrow Z$, then $I(X;Z|Y)=0$ i.e. $Y$ and $Z$ are conditionally independent given $Y$.
  **Pf.** from the probability formula we have
  $$I(X ; Z | Y )=E_{p(x, y, z)} \log \frac{p(X, Y | Z)}{p(X | Z) p(Y | Z)}$$

## Data Processing Inequality
é©¬å°”ç§‘å¤«ç³»ç»Ÿçš„ä¿¡æ¯æ˜¯å¦‚ä½•æ¼”åŒ–çš„? é©¬å°”å¯å¤«é“¾å®é™…ä¸Šç›¸å½“äºæ•°æ®åˆ†æ­¥å¤„ç†çš„è¿‡ç¨‹
Theorem : If  $X \rightarrow Y \rightarrow Z$, Then $I(X;Y)\ge I(X;Z)$
ä¿¡æ¯å¤„ç†å¾—è¶Šå¤š,ä¿¡æ¯ä¸¢å¤±çš„è¶Šå¤š
**Pf.** $I(X;Y,Z)=I(X;Z)+I(X;Y|Z)=I(X;Y)+I(X;Z|Y)$
Since $I(X;Z|Y)=0$, we have
$$I(X;Z)+I(X;Y|Z)=I(X;Y)$$

- In particular, if $Z=g(Y)$, then $I(X;Y)\ge I(X;g(Y))$
- Collary: If  $X \rightarrow Y \rightarrow Z$, $I(X;Y|Z)\le I(X;Y)$, å¯¹ä¸‰ä¸ªéšæœºå˜é‡è€Œè¨€, **æ¡ä»¶äº’ä¿¡æ¯ä¸ä¸€å®šå°äºç­‰äºäº’ä¿¡æ¯(ä¸æ¡ä»¶ç†µä¸åŒ)**

### I(X;Y;Z)
æœ‰å…³ä¸Šé¢Remarkçš„ä¸€ä¸ªåä¾‹:

Assume $X,Y$ are two independent random variables uniformly distributed on {0,1}.
  $$Z=X+Y (mod 2)$$
  We can find that $I(X;Y|Z)> I(X;Y)$. 

ä»é—®é¢˜ä¸­, $X,Y,Z$ä»»æ„ä¸¤ä¸ªéƒ½èƒ½å†³å®šå‰©ä¸‹ä¸€ä¸ª, åˆ†å¸ƒéƒ½ç›¸åŒ, ä¸”ä¸¤ä¸¤ç›¸äº’ç‹¬ç«‹(by def $p(X,Z)=p(X)p(Z)$).

$$\begin{aligned}
I(X ; Y | Z) &=H(X | Z)-H(X | Y, Z) \\
&=H(X | Z) \\
&=H(X) \\
&=1
\end{aligned}$$
$$1=I(X;Y|Z)>I(X;Y)=0$$
**Intuition:** å½“ä½ çŸ¥é“$Z$ä¹‹å,$X$å’Œ$Y$ä¹‹é—´å¯ä»¥è§£å‡ºæ›´å¤šä¿¡æ¯å‡ºæ¥. ç†µå°±ä¸å…·å¤‡è¿™ä¸€æ€§è´¨.
**Denote:** $I(X;Y;Z)=I(X;Y)-I(X;Y|Z)$ ä»…ä»…æ˜¯ç¬¦å·æ„ä¹‰, ä¸å…·æœ‰äº’ä¿¡æ¯çš„æ„ä¹‰, å› ä¸ºå®ƒå¯èƒ½å°äº0.

## Information Diagram

### 2RVs to more

| 2RVs | 3RVs| 4RVs
| --| --| --|
|![](./img/0304-2.png) | ![](./img/0311-1.png)| ![](./img/0311-2.png)
| **Virtual Circles** | Area are all nonnegative except $I(X;Y;Z)$| All areas can be expressed in combinations of (conditional) entropy/mutual info  |

Circles are not representing any entities, for independent variables, unintersected circles are not allowed, since values can be negative.
Only items like $H(X|Y)$, $I(X;Y|Z)$ are nonnegative
Reference: Ch. 3, Information Theory and Network Coding, R. W. Yeung

### Markov Chain

| $X\rightarrow Y\rightarrow Z$ | $X_1\rightarrow \cdots \rightarrow X_n$ |
|--|--|
|![](./img/0311-3.png) | ![](./img/0311-4.png)|
|ç”¨åŠåœ†è¡¨ç¤º,ä¿è¯ç›¸äº’ç›¸äº¤,**ä¿è¯æ¯å—éè´Ÿ**  | æ›´ä¸€èˆ¬çš„æƒ…å†µ, nä¸ªç›¸äº’ç›¸äº¤çš„åŠåœ†|
|å…±6å—,**ä¸‰è€…äº’ä¿¡æ¯=Xä¸Zçš„äº’ä¿¡æ¯** |ä¿è¯ç¬¬ä¸€ä¸ªå’Œæœ€åä¸€ä¸ªæœ‰æ˜æ˜¾ç›¸äº¤ |

### Examples: Use Info Diagram to Prove Inequalities

![](./img/0311-5.png)

$$\begin{array}{c}
H(X, Y, Z) \leq \frac{H(X, Y)+H(Y, Z)+H(Z, X)}{2} \leq H(X)+H(Y)+H(Z) \\
H(X | Y, Z)+H(Y | X, Z)+H(Z | X, Y) \leq \frac{H(X, Y | Z)+H(Y, Z | X)+H(Z, X | Y)}{2} \leq H(X, Y, Z)
\end{array}$$
With graphical interpretation:
$$a+c+e \le \frac{(a+b+c)+(c+d+e)+(a+f+e)}{2} \le a+b+\cdot +g $$
***Note, some areas can be negative! Take signals into account***

## Practical Examples

### Causality(å› æœæ¨æ–­)
æˆ‘ä»¬å°†ç³»ç»Ÿä¸­çš„å› ç´ ç”¨å›¾è¡¨ç¤º,è®¡ç®—ä¿¡æ¯é‡,å†™å‡ºä¿¡æ¯é‡ä¹‹é—´çš„ç›¸äº’å…³ç³»,æ¨å¯¼ä¿¡æ¯ä¹‹é—´æ˜¯å¦å…·æœ‰ä¸€å®šå…³ç³»
e.g. Given: $X \perp Y | Z$ and $X \perp Z$ and Prove: $X \perp Y$

### Perfect Secrecy(å®Œç¾å®‰å…¨æ¨¡å‹)
![](./img/0311-6.png)
æ˜æ–‡ç”±ç§˜å¯†å’Œå¯†é’¥ç”Ÿæˆ: $H(Y|X,Z)=0$
æ¥æ”¶ç«¯å¯ä»¥é€šè¿‡æ˜æ–‡å’Œå¯†é’¥ç”Ÿæˆ:$H(X|Y,Z)=0$
æˆ‘ä»¬å¯ä»¥ç”±æ­¤æ¨å‡º: $I(X;Y)\ge H(X)-H(Z)$
å‡è®¾çªƒå¬è€…ä¸ç§˜å¯†ä¹‹é—´æ¯«æ— å…³è”$I(X;Y) = 0$
é‚£ä¹ˆå¦‚æœè¦ä½¿$I(X;Y)=0$, æˆ‘ä»¬éœ€è¦$H(X)\le H(Z)$, å³ä¿¡æ¯é•¿åº¦å°äºå¯†é’¥é•¿åº¦.


## Fano's Inequality: Estimation

### Background

- Suppose that we wish to estimate a random variable ğ‘‹with a distribution $ğ‘(ğ‘¥)$.
- We observe a random variable $ğ‘Œ$ that is related to ğ‘‹by the conditional distribution $ğ‘(ğ‘¦|ğ‘¥)$.
- From ğ‘Œ, we calculate a function $ğ‘”(ğ‘Œ)=\hat{X}$, where $\hat{X}$ ğ‘‹is an estimate of ğ‘‹and takes on values in $\hat{X}$.
  - We will not restrict the alphabet $\hat{X}$ to be equal to ğ‘‹, and we will also allow the function ğ‘”(ğ‘Œ)to be random.
- We wish to bound the probability that $\hat{X}$â‰ ğ‘‹. We observe that ğ‘‹â†’ğ‘Œâ†’$\hat{X}$ forms a Markov chain. Define the probability of error
  $P_e=Pr(\hat{X} \neq X)$
- When ğ»(ğ‘‹|ğ‘Œ)=0, we know that $P_e=0$. How about $ğ»(ğ‘‹|ğ‘Œ)$,as $P_e\rightarrow 0$?

> **Theorem (Fanoâ€™s Inequality)** For any estimator $\hat{X}$ such that $X \rightarrow Y \rightarrow \hat{X}$ with $P_e = \Pr(\hat{X}\neq X)$ we have
> $$H\left(P_{e}\right)+P_{e} \log |\mathcal{X}| \geq H(X | \hat{X}) \geqslant H(X | Y)$$
> Or can be weakened to
> $$1+P_{e} \log |\mathcal{X}| \geqslant H(X | Y) \text{ or } P_{e} \geqslant \frac{H(X| Y)-1}{\log |x|}$$
> 
> åè€…æ˜¯data-processing ä¸ç­‰å¼, å‰è€…æ˜¯æ³•è¯ºä¸ç­‰å¼çš„æ ¸å¿ƒéƒ¨åˆ†.

### Proof

Define an error random variable 
$$E=\left\{\begin{array}{ll}
0, & \text { if } \hat{X}=X \\
1, & \text { if } \hat{X} \neq X
\end{array}\right.$$
Then
$$\begin{aligned}
H(E, X | \hat{X}) &=H(X | \hat{X})+H(E | X, \bar{X}) \\
&=H(E | \hat{X})+H(X | E, \hat{X})
\end{aligned}$$

- é©¬å°”å¯å¤«é“¾implies$H(E | X, \hat{X})=0$
- ç¬¬äºŒæ­¥,  $H(X|\hat{X},E=1)<=H(x)=H(P_e)$, ç†µæ°¸è¿œå°äºå­—æ¯è¡¨çš„å¯¹æ•°å€¼
- æ­¤å¤–, $H(X | E, \hat{X}) \leq P_{e} \log |x|$ å› ä¸º
$$\begin{aligned}
H(X | E, \hat{X}) &=\Pr(E=0) H(X | \hat{X}, E=0)+\Pr(E=1) H(X | \hat{X}, E=1) \\
& \leq\left(1-P_{e}\right) 0+P_{e} \log |\mathcal{X}|
\end{aligned}$$

**Corollary** Let $P_{e}=\Pr(X \neq \hat{X})$, and let $\hat{X}: y \rightarrow x$; then $H\left(P_{e}\right)+P_{e} \log (|x|-1) \geq H(X | Y)$, ç”±äºå·²çŸ¥$X$å’Œ$\hat{X}$ä¸ç­‰, åœ¨ä¼°è®¡æ—¶, ç†µçš„ä¸Šç•Œå¯ä»¥è°ƒå° (corollary)

ç›´è§‚ç†è§£:
$P_e \rightarrow 0$ implies $H(P_e) \rightarrow 0$ implies $H(X|Y)\rightarrow 0$ é”™è¯¯ç‡è¶‹å‘äº0æ—¶, Xå’ŒYçš„å…³ç³»è¶‹å‘ç¡®å®š.

Recall: binary entropy function
$H(p)=-plogp-(1-p)log(1-p)$ å®é™…æ˜¯ç®€å†™çš„è®°å·, è®¡ç®—çš„æ˜¯ä¸¤ç‚¹åˆ†å¸ƒçš„ç†µ

## Convexity/ Concavity of Information Measures

**Log Sum Inequality** for nonnegative $a_1, \cdots, a_n$ and $b_1, \cdots, b_n$,
$$\sum_{i=1}^{n} a_{i} \log \frac{a_{i}}{b_{i}} \geq\left(\sum_{i=1}^{n} a_{i}\right) \log \frac{\sum_{i=1}^{n} a_{i}}{\sum_{i=1}^{n} b_{i}}$$
with equality if and only if $\frac{a_i}{b_i}=const$.
Pf. by moving $\left(\sum_{i=1}^{n} a_{i}\right)$ to the left, the coefficient can be regarded as a probability distribution.

Corollaries:
- Concavity of $H(P)$
- $(X,Y)~p(x,y)=p(x)p(y|x)$, then $I(X;Y)$is a concave function of $p(x)$ for fixed $p(y|x)$ and a convex function of$p(y|x)$ for fixed $p(x)$ 
  - Note given $p(x)$, $p(y|x) \rightarrow p(x,y) \rightarrowp(y)$
- Convexity of relative entropy. $D(p\|q)$is a convex function for pair $(p,q)$.
- å¯ä»¥æŠŠ $p(x), p(y)$ç­‰çœ‹ä½œé«˜ç»´ç©ºé—´ä¸Šçš„ä¸€ä¸ªç‚¹, è€Œä¸æ˜¯æ¦‚ç‡åˆ†å¸ƒ. for $X ={1, 2,3...,n}$, define $\mathbb{x}_1 = p_1, \mathbb{x}_2= p_2, ..$
